{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RNN Model Evaluation Notebook\n",
        "\n",
        "This notebook provides comprehensive evaluation of the trained RNN models from the BARYONYX Lost & Found system.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The BARYONYX system uses three RNN models:\n",
        "\n",
        "1. **UserBehaviorLSTM**: Predicts user's next likely action based on behavior sequences\n",
        "2. **BidirectionalDescriptionRNN**: Classifies item descriptions into categories\n",
        "3. **TemporalPatternRNN**: Predicts optimal time periods for finding items\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Setup and Imports](#setup)\n",
        "2. [Load Trained Models](#load)\n",
        "3. [User Behavior LSTM Evaluation](#behavior)\n",
        "4. [Description RNN Evaluation](#description)\n",
        "5. [Temporal Pattern RNN Evaluation](#temporal)\n",
        "6. [Performance Analysis](#analysis)\n",
        "7. [Visualization and Results](#visualization)\n",
        "8. [Model Comparison](#comparison)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports {#setup}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Machine Learning imports\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    mean_squared_error, mean_absolute_error, r2_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.append('.')\n",
        "\n",
        "# Import project modules\n",
        "from rnn_models import RNNModelManager, UserBehaviorLSTM, BidirectionalDescriptionRNN, TemporalPatternRNN\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Trained Models {#load}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize RNN manager\n",
        "print(\"Loading RNN models from models/rnn_models/...\")\n",
        "rnn_manager = RNNModelManager(device=str(device))\n",
        "\n",
        "# Check if model files exist\n",
        "model_dir = 'models/rnn_models'\n",
        "model_files = {\n",
        "    'user_behavior_lstm.pth': 'User Behavior LSTM',\n",
        "    'description_birnn.pth': 'Description Bidirectional RNN', \n",
        "    'temporal_pattern_rnn.pth': 'Temporal Pattern RNN',\n",
        "    'vocab.pkl': 'Vocabulary',\n",
        "    'temporal_data.pkl': 'Temporal Data'\n",
        "}\n",
        "\n",
        "print(\"\\n=== Model File Status ===\")\n",
        "for filename, description in model_files.items():\n",
        "    filepath = os.path.join(model_dir, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        file_size = os.path.getsize(filepath) / 1024  # KB\n",
        "        print(f\"‚úÖ {description}: {filename} ({file_size:.1f} KB)\")\n",
        "    else:\n",
        "        print(f\"‚ùå {description}: {filename} - NOT FOUND\")\n",
        "\n",
        "# Load models\n",
        "try:\n",
        "    rnn_manager.load_models()\n",
        "    print(\"\\n‚úÖ RNN Models loaded successfully!\")\n",
        "    \n",
        "    # Check model status\n",
        "    print(f\"\\n=== Model Status ===\")\n",
        "    print(f\"User Behavior Model: {'‚úÖ Loaded' if hasattr(rnn_manager, 'user_behavior_model') else '‚ùå Not loaded'}\")\n",
        "    print(f\"Description Model: {'‚úÖ Loaded' if hasattr(rnn_manager, 'description_model') else '‚ùå Not loaded'}\")\n",
        "    print(f\"Temporal Model: {'‚úÖ Loaded' if hasattr(rnn_manager, 'temporal_model') else '‚ùå Not loaded'}\")\n",
        "    print(f\"Vocabulary: {'‚úÖ Loaded' if hasattr(rnn_manager, 'vocab') and rnn_manager.vocab else '‚ùå Not loaded'}\")\n",
        "    print(f\"Vocabulary Size: {len(rnn_manager.vocab) if hasattr(rnn_manager, 'vocab') and rnn_manager.vocab else 0}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading RNN models: {e}\")\n",
        "    print(\"Will proceed with evaluation using default models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. User Behavior LSTM Evaluation {#behavior}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User Behavior LSTM Evaluation\n",
        "def evaluate_user_behavior_model(rnn_manager, num_test_samples=200):\n",
        "    \"\"\"Evaluate the User Behavior LSTM model\"\"\"\n",
        "    \n",
        "    print(\"üß† Evaluating User Behavior LSTM...\")\n",
        "    \n",
        "    # Create test data\n",
        "    actions = ['search', 'upload', 'view', 'browse', 'logout']\n",
        "    item_types = ['phone', 'wallet', 'keys', 'laptop', 'glasses', 'watch', 'bag', 'book', 'charger', 'headphones']\n",
        "    \n",
        "    # Generate test sequences\n",
        "    test_sequences = []\n",
        "    test_labels = []\n",
        "    \n",
        "    for i in range(num_test_samples):\n",
        "        # Create a realistic user behavior sequence\n",
        "        sequence_length = 10\n",
        "        features = []\n",
        "        \n",
        "        for j in range(sequence_length):\n",
        "            # Generate realistic features\n",
        "            hour = np.random.uniform(0, 1)  # Normalized hour\n",
        "            day_of_week = np.random.uniform(0, 1)  # Normalized day\n",
        "            action_type = np.random.uniform(0, 1)  # Normalized action\n",
        "            item_type = np.random.uniform(0, 1) if j % 3 == 0 else 0  # Some actions have items\n",
        "            confidence = np.random.uniform(0.6, 1.0)\n",
        "            search_count = np.random.uniform(0, 1)\n",
        "            upload_count = np.random.uniform(0, 0.5)\n",
        "            view_count = np.random.uniform(0, 0.8)\n",
        "            time_since_last = np.random.uniform(0, 1)\n",
        "            session_length = np.random.uniform(0, 1)\n",
        "            \n",
        "            feature_vector = [hour, day_of_week, action_type, item_type, confidence,\n",
        "                            search_count, upload_count, view_count, time_since_last, session_length]\n",
        "            features.append(feature_vector)\n",
        "        \n",
        "        # Create label (next action)\n",
        "        next_action = np.random.choice(actions)\n",
        "        label = actions.index(next_action)\n",
        "        \n",
        "        test_sequences.append(features)\n",
        "        test_labels.append(label)\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_test = torch.FloatTensor(test_sequences).to(device)\n",
        "    y_test = torch.LongTensor(test_labels).to(device)\n",
        "    \n",
        "    # Evaluate model\n",
        "    model = rnn_manager.user_behavior_model\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "    attention_weights = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(X_test), 32):  # Batch processing\n",
        "            batch_X = X_test[i:i+32]\n",
        "            outputs, attention = model(batch_X)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "            \n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            probabilities.extend(probs.cpu().numpy())\n",
        "            attention_weights.extend(attention.cpu().numpy())\n",
        "    \n",
        "    predictions = np.array(predictions)\n",
        "    y_test_np = y_test.cpu().numpy()\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test_np, predictions)\n",
        "    precision = precision_score(y_test_np, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test_np, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test_np, predictions, average='weighted', zero_division=0)\n",
        "    \n",
        "    print(f\"\\n=== User Behavior LSTM Results ===\")\n",
        "    print(f\"Test samples: {len(test_sequences)}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    \n",
        "    # Per-class performance\n",
        "    print(f\"\\nPer-class Performance:\")\n",
        "    for i, action in enumerate(actions):\n",
        "        class_mask = y_test_np == i\n",
        "        if np.sum(class_mask) > 0:\n",
        "            class_acc = accuracy_score(y_test_np[class_mask], predictions[class_mask])\n",
        "            print(f\"  {action}: {class_acc:.4f} ({np.sum(class_mask)} samples)\")\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test_np, predictions)\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(\"Predicted ->\", end=\"\")\n",
        "    for action in actions:\n",
        "        print(f\"{action:>8}\", end=\"\")\n",
        "    print()\n",
        "    for i, action in enumerate(actions):\n",
        "        print(f\"{action:>8}\", end=\"\")\n",
        "        for j in range(len(actions)):\n",
        "            print(f\"{cm[i,j]:>8}\", end=\"\")\n",
        "        print()\n",
        "    \n",
        "    return {\n",
        "        'predictions': predictions,\n",
        "        'true_labels': y_test_np,\n",
        "        'probabilities': probabilities,\n",
        "        'attention_weights': attention_weights,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "# Run evaluation\n",
        "behavior_results = evaluate_user_behavior_model(rnn_manager)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Description RNN Evaluation {#description}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Description RNN Evaluation\n",
        "def evaluate_description_model(rnn_manager, num_test_samples=200):\n",
        "    \"\"\"Evaluate the Description Bidirectional RNN model\"\"\"\n",
        "    \n",
        "    print(\"üìù Evaluating Description Bidirectional RNN...\")\n",
        "    \n",
        "    # Test descriptions for different categories\n",
        "    test_descriptions = [\n",
        "        # Phone\n",
        "        \"Lost black iPhone 12 with cracked screen\",\n",
        "        \"Found Samsung Galaxy phone with blue case\", \n",
        "        \"Lost iPhone 13 Pro Max gold\",\n",
        "        \"Found Google Pixel phone black\",\n",
        "        \"Lost OnePlus phone with red case\",\n",
        "        \n",
        "        # Mouse\n",
        "        \"Lost computer mouse wireless black\",\n",
        "        \"Found gaming mouse red with RGB\",\n",
        "        \"Lost wireless mouse Logitech black\",\n",
        "        \"Found optical mouse white\",\n",
        "        \"Lost Bluetooth mouse silver\",\n",
        "        \n",
        "        # Wallet\n",
        "        \"Lost wallet brown leather with cards\",\n",
        "        \"Found wallet black leather bifold\",\n",
        "        \"Lost wallet red leather with money\",\n",
        "        \"Found wallet blue canvas\",\n",
        "        \"Lost wallet black synthetic leather\",\n",
        "        \n",
        "        # Tumbler\n",
        "        \"Lost tumbler stainless steel silver\",\n",
        "        \"Found water bottle blue plastic\",\n",
        "        \"Lost tumbler black with handle\",\n",
        "        \"Found coffee cup white ceramic\",\n",
        "        \"Lost tumbler red with straw\"\n",
        "    ]\n",
        "    \n",
        "    # Create more test samples by generating variations\n",
        "    all_descriptions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    categories = {\n",
        "        0: 'phone',\n",
        "        1: 'mouse', \n",
        "        2: 'wallet',\n",
        "        3: 'tumbler'\n",
        "    }\n",
        "    \n",
        "    # Generate test samples\n",
        "    for i in range(num_test_samples):\n",
        "        if i < len(test_descriptions):\n",
        "            desc = test_descriptions[i]\n",
        "            label = i // 5  # 5 descriptions per category\n",
        "        else:\n",
        "            # Generate random descriptions for the 4 categories\n",
        "            item_types = ['phone', 'mouse', 'wallet', 'tumbler']\n",
        "            colors = ['black', 'blue', 'red', 'white', 'silver', 'brown']\n",
        "            item_type = np.random.choice(item_types)\n",
        "            color = np.random.choice(colors)\n",
        "            desc = f\"Lost {color} {item_type}\"\n",
        "            label = item_types.index(item_type)\n",
        "        \n",
        "        all_descriptions.append(desc)\n",
        "        all_labels.append(label)\n",
        "    \n",
        "    # Prepare data for model\n",
        "    if not hasattr(rnn_manager, 'vocab') or not rnn_manager.vocab:\n",
        "        print(\"Building vocabulary from test data...\")\n",
        "        from collections import Counter\n",
        "        counter = Counter()\n",
        "        for desc in all_descriptions:\n",
        "            counter.update(desc.lower().split())\n",
        "        rnn_manager.vocab = {w: i + 1 for i, (w, c) in enumerate(counter.items()) if c >= 1}\n",
        "        rnn_manager.vocab_size = len(rnn_manager.vocab)\n",
        "        rnn_manager.vocab['<PAD>'] = 0\n",
        "        rnn_manager.vocab['<UNK>'] = len(rnn_manager.vocab)\n",
        "    \n",
        "    # Convert texts to sequences\n",
        "    def text_to_sequence(text, max_length=20):\n",
        "        words = text.lower().split()\n",
        "        word_indices = [rnn_manager.vocab.get(w, rnn_manager.vocab['<UNK>']) for w in words]\n",
        "        \n",
        "        if len(word_indices) < max_length:\n",
        "            word_indices.extend([rnn_manager.vocab['<PAD>']] * (max_length - len(word_indices)))\n",
        "        else:\n",
        "            word_indices = word_indices[:max_length]\n",
        "        \n",
        "        return word_indices\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_test = []\n",
        "    for desc in all_descriptions:\n",
        "        X_test.append(text_to_sequence(desc))\n",
        "    \n",
        "    X_test = torch.LongTensor(X_test).to(device)\n",
        "    y_test = torch.LongTensor(all_labels).to(device)\n",
        "    \n",
        "    # Evaluate model\n",
        "    model = rnn_manager.description_model\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "    attention_weights = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(X_test), 32):  # Batch processing\n",
        "            batch_X = X_test[i:i+32]\n",
        "            outputs, attention = model(batch_X)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "            \n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            probabilities.extend(probs.cpu().numpy())\n",
        "            attention_weights.extend(attention.cpu().numpy())\n",
        "    \n",
        "    predictions = np.array(predictions)\n",
        "    y_test_np = y_test.cpu().numpy()\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test_np, predictions)\n",
        "    precision = precision_score(y_test_np, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test_np, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test_np, predictions, average='weighted', zero_division=0)\n",
        "    \n",
        "    print(f\"\\n=== Description RNN Results ===\")\n",
        "    print(f\"Test samples: {len(all_descriptions)}\")\n",
        "    print(f\"Vocabulary size: {len(rnn_manager.vocab)}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    \n",
        "    # Per-class performance\n",
        "    print(f\"\\nPer-class Performance:\")\n",
        "    for i, (cat_id, cat_name) in enumerate(categories.items()):\n",
        "        class_mask = y_test_np == cat_id\n",
        "        if np.sum(class_mask) > 0:\n",
        "            class_acc = accuracy_score(y_test_np[class_mask], predictions[class_mask])\n",
        "            print(f\"  {cat_name}: {class_acc:.4f} ({np.sum(class_mask)} samples)\")\n",
        "    \n",
        "    # Show some example predictions\n",
        "    print(f\"\\nExample Predictions:\")\n",
        "    for i in range(min(10, len(all_descriptions))):\n",
        "        pred_cat = categories.get(predictions[i], 'unknown')\n",
        "        true_cat = categories.get(y_test_np[i], 'unknown')\n",
        "        correct = \"‚úÖ\" if predictions[i] == y_test_np[i] else \"‚ùå\"\n",
        "        print(f\"  {correct} '{all_descriptions[i][:30]}...' -> Pred: {pred_cat}, True: {true_cat}\")\n",
        "    \n",
        "    return {\n",
        "        'predictions': predictions,\n",
        "        'true_labels': y_test_np,\n",
        "        'probabilities': probabilities,\n",
        "        'attention_weights': attention_weights,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'descriptions': all_descriptions,\n",
        "        'categories': categories\n",
        "    }\n",
        "\n",
        "# Run evaluation\n",
        "description_results = evaluate_description_model(rnn_manager)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Temporal Pattern RNN Evaluation {#temporal}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Temporal Pattern RNN Evaluation\n",
        "def evaluate_temporal_model(rnn_manager, num_test_samples=200):\n",
        "    \"\"\"Evaluate the Temporal Pattern RNN model\"\"\"\n",
        "    \n",
        "    print(\"‚è∞ Evaluating Temporal Pattern RNN...\")\n",
        "    \n",
        "    # Generate test temporal data\n",
        "    test_sequences = []\n",
        "    test_labels = []\n",
        "    \n",
        "    for i in range(num_test_samples):\n",
        "        # Create temporal sequence (e.g., 7 days of data)\n",
        "        sequence_length = 7\n",
        "        features = []\n",
        "        \n",
        "        for day in range(sequence_length):\n",
        "            # Generate realistic temporal features\n",
        "            hour = np.random.uniform(0, 1)  # Normalized hour\n",
        "            day_of_week = day / 7.0  # Normalized day of week\n",
        "            month = np.random.uniform(0, 1)  # Normalized month\n",
        "            season = np.random.uniform(0, 1)  # Normalized season\n",
        "            weather = np.random.uniform(0, 1)  # Normalized weather\n",
        "            location_type = np.random.uniform(0, 1)  # Normalized location type\n",
        "            item_category = np.random.uniform(0, 1)  # Normalized item category\n",
        "            user_activity = np.random.uniform(0, 1)  # Normalized user activity\n",
        "            search_frequency = np.random.uniform(0, 1)  # Normalized search frequency\n",
        "            success_rate = np.random.uniform(0, 1)  # Normalized success rate\n",
        "            \n",
        "            feature_vector = [hour, day_of_week, month, season, weather, \n",
        "                            location_type, item_category, user_activity, \n",
        "                            search_frequency, success_rate]\n",
        "            features.append(feature_vector)\n",
        "        \n",
        "        # Create label (optimal time period for finding items)\n",
        "        # 0: early_morning, 1: morning, 2: afternoon, 3: evening, 4: night\n",
        "        optimal_period = np.random.randint(0, 5)\n",
        "        \n",
        "        test_sequences.append(features)\n",
        "        test_labels.append(optimal_period)\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_test = torch.FloatTensor(test_sequences).to(device)\n",
        "    y_test = torch.LongTensor(test_labels).to(device)\n",
        "    \n",
        "    # Evaluate model\n",
        "    model = rnn_manager.temporal_model\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(X_test), 32):  # Batch processing\n",
        "            batch_X = X_test[i:i+32]\n",
        "            outputs = model(batch_X)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "            \n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            probabilities.extend(probs.cpu().numpy())\n",
        "    \n",
        "    predictions = np.array(predictions)\n",
        "    y_test_np = y_test.cpu().numpy()\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test_np, predictions)\n",
        "    precision = precision_score(y_test_np, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test_np, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test_np, predictions, average='weighted', zero_division=0)\n",
        "    \n",
        "    time_periods = {\n",
        "        0: 'Early Morning (6-9 AM)',\n",
        "        1: 'Morning (9-12 PM)', \n",
        "        2: 'Afternoon (12-5 PM)',\n",
        "        3: 'Evening (5-9 PM)',\n",
        "        4: 'Night (9 PM-6 AM)'\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n=== Temporal Pattern RNN Results ===\")\n",
        "    print(f\"Test samples: {len(test_sequences)}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    \n",
        "    # Per-class performance\n",
        "    print(f\"\\nPer-class Performance:\")\n",
        "    for i, (period_id, period_name) in enumerate(time_periods.items()):\n",
        "        class_mask = y_test_np == period_id\n",
        "        if np.sum(class_mask) > 0:\n",
        "            class_acc = accuracy_score(y_test_np[class_mask], predictions[class_mask])\n",
        "            print(f\"  {period_name}: {class_acc:.4f} ({np.sum(class_mask)} samples)\")\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test_np, predictions)\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(\"Predicted ->\", end=\"\")\n",
        "    for period_name in time_periods.values():\n",
        "        print(f\"{period_name[:12]:>12}\", end=\"\")\n",
        "    print()\n",
        "    for i, (period_id, period_name) in enumerate(time_periods.items()):\n",
        "        print(f\"{period_name[:12]:>12}\", end=\"\")\n",
        "        for j in range(len(time_periods)):\n",
        "            print(f\"{cm[i,j]:>12}\", end=\"\")\n",
        "        print()\n",
        "    \n",
        "    # Show some example predictions\n",
        "    print(f\"\\nExample Predictions:\")\n",
        "    for i in range(min(10, len(test_sequences))):\n",
        "        pred_period = time_periods.get(predictions[i], 'unknown')\n",
        "        true_period = time_periods.get(y_test_np[i], 'unknown')\n",
        "        correct = \"‚úÖ\" if predictions[i] == y_test_np[i] else \"‚ùå\"\n",
        "        print(f\"  {correct} Sample {i+1} -> Pred: {pred_period}, True: {true_period}\")\n",
        "    \n",
        "    return {\n",
        "        'predictions': predictions,\n",
        "        'true_labels': y_test_np,\n",
        "        'probabilities': probabilities,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'confusion_matrix': cm,\n",
        "        'time_periods': time_periods\n",
        "    }\n",
        "\n",
        "# Run evaluation\n",
        "temporal_results = evaluate_temporal_model(rnn_manager)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Performance Analysis {#analysis}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance Analysis\n",
        "def analyze_model_performance(behavior_results, description_results, temporal_results):\n",
        "    \"\"\"Analyze and compare performance across all RNN models\"\"\"\n",
        "    \n",
        "    print(\"üìä RNN Model Performance Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Create performance summary\n",
        "    models = {\n",
        "        'User Behavior LSTM': {\n",
        "            'accuracy': behavior_results['accuracy'],\n",
        "            'precision': behavior_results['precision'],\n",
        "            'recall': behavior_results['recall'],\n",
        "            'f1': behavior_results['f1'],\n",
        "            'type': 'Sequence Classification',\n",
        "            'purpose': 'Predict next user action'\n",
        "        },\n",
        "        'Description RNN': {\n",
        "            'accuracy': description_results['accuracy'],\n",
        "            'precision': description_results['precision'],\n",
        "            'recall': description_results['recall'],\n",
        "            'f1': description_results['f1'],\n",
        "            'type': 'Text Classification',\n",
        "            'purpose': 'Classify item descriptions'\n",
        "        },\n",
        "        'Temporal Pattern RNN': {\n",
        "            'accuracy': temporal_results['accuracy'],\n",
        "            'precision': temporal_results['precision'],\n",
        "            'recall': temporal_results['recall'],\n",
        "            'f1': temporal_results['f1'],\n",
        "            'type': 'Time Series Classification',\n",
        "            'purpose': 'Predict optimal search times'\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Performance summary table\n",
        "    print(\"\\nüìà Performance Summary\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Model':<25} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for model_name, metrics in models.items():\n",
        "        print(f\"{model_name:<25} {metrics['accuracy']:<10.4f} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f} {metrics['f1']:<10.4f}\")\n",
        "    \n",
        "    # Best performing model\n",
        "    best_model = max(models.items(), key=lambda x: x[1]['accuracy'])\n",
        "    print(f\"\\nüèÜ Best Performing Model: {best_model[0]}\")\n",
        "    print(f\"   Accuracy: {best_model[1]['accuracy']:.4f}\")\n",
        "    \n",
        "    # Model effectiveness analysis\n",
        "    print(f\"\\nüìã Model Effectiveness Analysis\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for model_name, metrics in models.items():\n",
        "        accuracy = metrics['accuracy']\n",
        "        f1 = metrics['f1']\n",
        "        \n",
        "        if accuracy >= 0.9:\n",
        "            performance = \"Excellent ‚≠ê‚≠ê‚≠ê\"\n",
        "        elif accuracy >= 0.8:\n",
        "            performance = \"Very Good ‚≠ê‚≠ê\"\n",
        "        elif accuracy >= 0.7:\n",
        "            performance = \"Good ‚≠ê\"\n",
        "        elif accuracy >= 0.6:\n",
        "            performance = \"Fair\"\n",
        "        else:\n",
        "            performance = \"Needs Improvement\"\n",
        "        \n",
        "        print(f\"{model_name}: {performance} (Acc: {accuracy:.3f}, F1: {f1:.3f})\")\n",
        "    \n",
        "    # Recommendations\n",
        "    print(f\"\\nüí° Recommendations\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    for model_name, metrics in models.items():\n",
        "        accuracy = metrics['accuracy']\n",
        "        if accuracy < 0.7:\n",
        "            print(f\"‚Ä¢ {model_name}: Consider retraining with more data or hyperparameter tuning\")\n",
        "        elif accuracy < 0.8:\n",
        "            print(f\"‚Ä¢ {model_name}: Good performance, minor improvements possible\")\n",
        "        else:\n",
        "            print(f\"‚Ä¢ {model_name}: Excellent performance, ready for production\")\n",
        "    \n",
        "    return models\n",
        "\n",
        "# Run performance analysis\n",
        "performance_summary = analyze_model_performance(behavior_results, description_results, temporal_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization and Results {#visualization}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "def create_evaluation_visualizations(behavior_results, description_results, temporal_results, performance_summary):\n",
        "    \"\"\"Create comprehensive visualizations for RNN model evaluation\"\"\"\n",
        "    \n",
        "    # Set up the plotting style\n",
        "    plt.style.use('default')\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "    \n",
        "    # 1. Model Performance Comparison\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    models = list(performance_summary.keys())\n",
        "    accuracies = [performance_summary[model]['accuracy'] for model in models]\n",
        "    f1_scores = [performance_summary[model]['f1'] for model in models]\n",
        "    \n",
        "    x = np.arange(len(models))\n",
        "    width = 0.35\n",
        "    \n",
        "    ax1.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
        "    ax1.bar(x + width/2, f1_scores, width, label='F1-Score', alpha=0.8)\n",
        "    ax1.set_xlabel('Models')\n",
        "    ax1.set_ylabel('Score')\n",
        "    ax1.set_title('Model Performance Comparison')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels([m.replace(' ', '\\n') for m in models], rotation=45, ha='right')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. User Behavior LSTM Confusion Matrix\n",
        "    ax2 = plt.subplot(3, 3, 2)\n",
        "    cm_behavior = behavior_results['confusion_matrix']\n",
        "    actions = ['search', 'upload', 'view', 'browse', 'logout']\n",
        "    sns.heatmap(cm_behavior, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=actions, yticklabels=actions, ax=ax2)\n",
        "    ax2.set_title('User Behavior LSTM\\nConfusion Matrix')\n",
        "    ax2.set_xlabel('Predicted')\n",
        "    ax2.set_ylabel('Actual')\n",
        "    \n",
        "    # 3. Description RNN Category Performance\n",
        "    ax3 = plt.subplot(3, 3, 3)\n",
        "    categories = description_results['categories']\n",
        "    cat_names = list(categories.values())\n",
        "    cat_accuracies = []\n",
        "    \n",
        "    for i, (cat_id, cat_name) in enumerate(categories.items()):\n",
        "        class_mask = description_results['true_labels'] == cat_id\n",
        "        if np.sum(class_mask) > 0:\n",
        "            class_acc = accuracy_score(description_results['true_labels'][class_mask], \n",
        "                                     description_results['predictions'][class_mask])\n",
        "            cat_accuracies.append(class_acc)\n",
        "        else:\n",
        "            cat_accuracies.append(0)\n",
        "    \n",
        "    bars = ax3.bar(range(len(cat_names)), cat_accuracies, alpha=0.8)\n",
        "    ax3.set_xlabel('Categories')\n",
        "    ax3.set_ylabel('Accuracy')\n",
        "    ax3.set_title('Description RNN\\nPer-Category Performance')\n",
        "    ax3.set_xticks(range(len(cat_names)))\n",
        "    ax3.set_xticklabels(cat_names, rotation=45, ha='right')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Color bars based on performance\n",
        "    for i, bar in enumerate(bars):\n",
        "        if cat_accuracies[i] >= 0.8:\n",
        "            bar.set_color('green')\n",
        "        elif cat_accuracies[i] >= 0.6:\n",
        "            bar.set_color('orange')\n",
        "        else:\n",
        "            bar.set_color('red')\n",
        "    \n",
        "    # 4. Temporal Pattern RNN Confusion Matrix\n",
        "    ax4 = plt.subplot(3, 3, 4)\n",
        "    cm_temporal = temporal_results['confusion_matrix']\n",
        "    time_periods = list(temporal_results['time_periods'].values())\n",
        "    time_labels = [period.split('(')[0].strip() for period in time_periods]\n",
        "    \n",
        "    sns.heatmap(cm_temporal, annot=True, fmt='d', cmap='Oranges',\n",
        "                xticklabels=time_labels, yticklabels=time_labels, ax=ax4)\n",
        "    ax4.set_title('Temporal Pattern RNN\\nConfusion Matrix')\n",
        "    ax4.set_xlabel('Predicted')\n",
        "    ax4.set_ylabel('Actual')\n",
        "    \n",
        "    # 5. Model Accuracy Distribution\n",
        "    ax5 = plt.subplot(3, 3, 5)\n",
        "    model_names = [m.replace(' ', '\\n') for m in models]\n",
        "    colors = ['skyblue', 'lightgreen', 'lightcoral']\n",
        "    wedges, texts, autotexts = ax5.pie(accuracies, labels=model_names, autopct='%1.1f%%', \n",
        "                                       colors=colors, startangle=90)\n",
        "    ax5.set_title('Model Accuracy\\nDistribution')\n",
        "    \n",
        "    # 6. Precision vs Recall Scatter\n",
        "    ax6 = plt.subplot(3, 3, 6)\n",
        "    precisions = [performance_summary[model]['precision'] for model in models]\n",
        "    recalls = [performance_summary[model]['recall'] for model in models]\n",
        "    \n",
        "    scatter = ax6.scatter(precisions, recalls, s=200, alpha=0.7, c=accuracies, \n",
        "                         cmap='viridis', edgecolors='black')\n",
        "    \n",
        "    for i, model in enumerate(models):\n",
        "        ax6.annotate(model.replace(' ', '\\n'), (precisions[i], recalls[i]), \n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "    \n",
        "    ax6.set_xlabel('Precision')\n",
        "    ax6.set_ylabel('Recall')\n",
        "    ax6.set_title('Precision vs Recall\\n(Size = Accuracy)')\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "    plt.colorbar(scatter, ax=ax6, label='Accuracy')\n",
        "    \n",
        "    # 7. Model Performance Radar Chart\n",
        "    ax7 = plt.subplot(3, 3, 7, projection='polar')\n",
        "    \n",
        "    # Metrics for radar chart\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "    angles += angles[:1]  # Complete the circle\n",
        "    \n",
        "    for i, model in enumerate(models):\n",
        "        values = [performance_summary[model]['accuracy'],\n",
        "                 performance_summary[model]['precision'],\n",
        "                 performance_summary[model]['recall'],\n",
        "                 performance_summary[model]['f1']]\n",
        "        values += values[:1]  # Complete the circle\n",
        "        \n",
        "        ax7.plot(angles, values, 'o-', linewidth=2, label=model)\n",
        "        ax7.fill(angles, values, alpha=0.25)\n",
        "    \n",
        "    ax7.set_xticks(angles[:-1])\n",
        "    ax7.set_xticklabels(metrics)\n",
        "    ax7.set_ylim(0, 1)\n",
        "    ax7.set_title('Model Performance\\nRadar Chart', pad=20)\n",
        "    ax7.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "    ax7.grid(True)\n",
        "    \n",
        "    # 8. Training Data Distribution (if available)\n",
        "    ax8 = plt.subplot(3, 3, 8)\n",
        "    \n",
        "    # Simulate training data distribution\n",
        "    sample_sizes = [750, 800, 600]  # From the training output\n",
        "    model_labels = ['User\\nBehavior', 'Description', 'Temporal']\n",
        "    \n",
        "    bars = ax8.bar(model_labels, sample_sizes, alpha=0.8, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
        "    ax8.set_ylabel('Training Samples')\n",
        "    ax8.set_title('Training Data\\nDistribution')\n",
        "    ax8.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, size in zip(bars, sample_sizes):\n",
        "        ax8.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
        "                str(size), ha='center', va='bottom')\n",
        "    \n",
        "    # 9. Model Complexity vs Performance\n",
        "    ax9 = plt.subplot(3, 3, 9)\n",
        "    \n",
        "    # Simulate model complexity (parameters)\n",
        "    complexity = [50000, 75000, 40000]  # Estimated parameters\n",
        "    colors = ['red' if acc < 0.7 else 'orange' if acc < 0.8 else 'green' \n",
        "              for acc in accuracies]\n",
        "    \n",
        "    scatter = ax9.scatter(complexity, accuracies, s=300, c=colors, alpha=0.7, edgecolors='black')\n",
        "    \n",
        "    for i, model in enumerate(models):\n",
        "        ax9.annotate(model.replace(' ', '\\n'), (complexity[i], accuracies[i]), \n",
        "                    xytext=(10, 10), textcoords='offset points', fontsize=8)\n",
        "    \n",
        "    ax9.set_xlabel('Model Complexity (Parameters)')\n",
        "    ax9.set_ylabel('Accuracy')\n",
        "    ax9.set_title('Model Complexity\\nvs Performance')\n",
        "    ax9.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(\"\\nüìä Visualization Summary\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Total models evaluated: {len(models)}\")\n",
        "    print(f\"Average accuracy: {np.mean(accuracies):.3f}\")\n",
        "    print(f\"Best accuracy: {np.max(accuracies):.3f}\")\n",
        "    print(f\"Worst accuracy: {np.min(accuracies):.3f}\")\n",
        "    print(f\"Standard deviation: {np.std(accuracies):.3f}\")\n",
        "\n",
        "# Create visualizations\n",
        "create_evaluation_visualizations(behavior_results, description_results, temporal_results, performance_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Comparison and Final Report {#comparison}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive final report\n",
        "def generate_final_report(behavior_results, description_results, temporal_results, performance_summary):\n",
        "    \"\"\"Generate a comprehensive final evaluation report\"\"\"\n",
        "    \n",
        "    print(\"üéØ BARYONYX RNN Model Evaluation - Final Report\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Device Used: {device}\")\n",
        "    print()\n",
        "    \n",
        "    # Executive Summary\n",
        "    print(\"üìã EXECUTIVE SUMMARY\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    best_model = max(performance_summary.items(), key=lambda x: x[1]['accuracy'])\n",
        "    worst_model = min(performance_summary.items(), key=lambda x: x[1]['accuracy'])\n",
        "    avg_accuracy = np.mean([m['accuracy'] for m in performance_summary.values()])\n",
        "    \n",
        "    print(f\"‚Ä¢ Total Models Evaluated: {len(performance_summary)}\")\n",
        "    print(f\"‚Ä¢ Average Accuracy: {avg_accuracy:.3f}\")\n",
        "    print(f\"‚Ä¢ Best Performing Model: {best_model[0]} ({best_model[1]['accuracy']:.3f})\")\n",
        "    print(f\"‚Ä¢ Worst Performing Model: {worst_model[0]} ({worst_model[1]['accuracy']:.3f})\")\n",
        "    print(f\"‚Ä¢ Production Ready Models: {sum(1 for m in performance_summary.values() if m['accuracy'] >= 0.8)}\")\n",
        "    print()\n",
        "    \n",
        "    # Detailed Model Analysis\n",
        "    print(\"üîç DETAILED MODEL ANALYSIS\")\n",
        "    print(\"-\" * 35)\n",
        "    \n",
        "    for model_name, metrics in performance_summary.items():\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  Purpose: {metrics['purpose']}\")\n",
        "        print(f\"  Type: {metrics['type']}\")\n",
        "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
        "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
        "        print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
        "        \n",
        "        # Performance assessment\n",
        "        if metrics['accuracy'] >= 0.9:\n",
        "            status = \"üü¢ EXCELLENT - Ready for production\"\n",
        "        elif metrics['accuracy'] >= 0.8:\n",
        "            status = \"üü° GOOD - Minor improvements possible\"\n",
        "        elif metrics['accuracy'] >= 0.7:\n",
        "            status = \"üü† FAIR - Consider retraining\"\n",
        "        else:\n",
        "            status = \"üî¥ POOR - Needs significant improvement\"\n",
        "        \n",
        "        print(f\"  Status: {status}\")\n",
        "    \n",
        "    # Key Findings\n",
        "    print(f\"\\nüîç KEY FINDINGS\")\n",
        "    print(\"-\" * 20)\n",
        "    \n",
        "    findings = []\n",
        "    \n",
        "    # Check for high-performing models\n",
        "    excellent_models = [name for name, m in performance_summary.items() if m['accuracy'] >= 0.9]\n",
        "    if excellent_models:\n",
        "        findings.append(f\"‚Ä¢ {len(excellent_models)} model(s) achieved excellent performance (‚â•90%): {', '.join(excellent_models)}\")\n",
        "    \n",
        "    # Check for balanced performance\n",
        "    f1_scores = [m['f1'] for m in performance_summary.values()]\n",
        "    if np.std(f1_scores) < 0.1:\n",
        "        findings.append(\"‚Ä¢ Models show balanced precision and recall performance\")\n",
        "    else:\n",
        "        findings.append(\"‚Ä¢ Some models show imbalanced precision/recall - consider class weighting\")\n",
        "    \n",
        "    # Check for consistent performance\n",
        "    if np.std([m['accuracy'] for m in performance_summary.values()]) < 0.1:\n",
        "        findings.append(\"‚Ä¢ All models show consistent performance levels\")\n",
        "    else:\n",
        "        findings.append(\"‚Ä¢ Performance varies significantly across models - investigate training differences\")\n",
        "    \n",
        "    # Check for overfitting\n",
        "    for name, metrics in performance_summary.items():\n",
        "        if metrics['precision'] > metrics['recall'] + 0.1:\n",
        "            findings.append(f\"‚Ä¢ {name} may be overfitting (precision >> recall)\")\n",
        "            break\n",
        "    \n",
        "    for finding in findings:\n",
        "        print(finding)\n",
        "    \n",
        "    # Recommendations\n",
        "    print(f\"\\nüí° RECOMMENDATIONS\")\n",
        "    print(\"-\" * 25)\n",
        "    \n",
        "    recommendations = []\n",
        "    \n",
        "    # Model-specific recommendations\n",
        "    for name, metrics in performance_summary.items():\n",
        "        if metrics['accuracy'] < 0.7:\n",
        "            recommendations.append(f\"‚Ä¢ {name}: Retrain with more data, try different architectures, or adjust hyperparameters\")\n",
        "        elif metrics['accuracy'] < 0.8:\n",
        "            recommendations.append(f\"‚Ä¢ {name}: Consider data augmentation or ensemble methods\")\n",
        "        else:\n",
        "            recommendations.append(f\"‚Ä¢ {name}: Monitor performance in production, consider A/B testing\")\n",
        "    \n",
        "    # General recommendations\n",
        "    recommendations.extend([\n",
        "        \"‚Ä¢ Implement continuous monitoring for model drift\",\n",
        "        \"‚Ä¢ Set up automated retraining pipelines\",\n",
        "        \"‚Ä¢ Consider ensemble methods for critical predictions\",\n",
        "        \"‚Ä¢ Implement confidence thresholds for predictions\",\n",
        "        \"‚Ä¢ Regular evaluation with real-world data\"\n",
        "    ])\n",
        "    \n",
        "    for i, rec in enumerate(recommendations, 1):\n",
        "        print(f\"{i}. {rec}\")\n",
        "    \n",
        "    # Technical Specifications\n",
        "    print(f\"\\n‚öôÔ∏è TECHNICAL SPECIFICATIONS\")\n",
        "    print(\"-\" * 35)\n",
        "    print(f\"‚Ä¢ PyTorch Version: {torch.__version__}\")\n",
        "    print(f\"‚Ä¢ CUDA Available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"‚Ä¢ GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚Ä¢ Evaluation Samples: 200 per model\")\n",
        "    print(f\"‚Ä¢ Batch Size: 32\")\n",
        "    print(f\"‚Ä¢ Device: {device}\")\n",
        "    \n",
        "    # Model File Locations\n",
        "    print(f\"\\nüìÅ MODEL FILES\")\n",
        "    print(\"-\" * 20)\n",
        "    model_files = [\n",
        "        \"models/rnn_models/user_behavior_lstm.pth\",\n",
        "        \"models/rnn_models/description_birnn.pth\", \n",
        "        \"models/rnn_models/temporal_pattern_rnn.pth\",\n",
        "        \"models/rnn_models/vocab.pkl\",\n",
        "        \"models/rnn_models/temporal_data.pkl\"\n",
        "    ]\n",
        "    \n",
        "    for file_path in model_files:\n",
        "        if os.path.exists(file_path):\n",
        "            size = os.path.getsize(file_path) / 1024\n",
        "            print(f\"‚úÖ {file_path} ({size:.1f} KB)\")\n",
        "        else:\n",
        "            print(f\"‚ùå {file_path} - Not found\")\n",
        "    \n",
        "    # Conclusion\n",
        "    print(f\"\\nüéØ CONCLUSION\")\n",
        "    print(\"-\" * 15)\n",
        "    \n",
        "    production_ready = sum(1 for m in performance_summary.values() if m['accuracy'] >= 0.8)\n",
        "    total_models = len(performance_summary)\n",
        "    \n",
        "    if production_ready == total_models:\n",
        "        conclusion = \"All RNN models are performing excellently and are ready for production deployment.\"\n",
        "    elif production_ready >= total_models * 0.7:\n",
        "        conclusion = \"Most RNN models are performing well. Consider minor improvements for underperforming models.\"\n",
        "    else:\n",
        "        conclusion = \"Several models need improvement before production deployment. Focus on retraining and optimization.\"\n",
        "    \n",
        "    print(conclusion)\n",
        "    print(f\"\\nOverall System Readiness: {production_ready}/{total_models} models production-ready\")\n",
        "    \n",
        "    return {\n",
        "        'summary': {\n",
        "            'total_models': len(performance_summary),\n",
        "            'avg_accuracy': avg_accuracy,\n",
        "            'best_model': best_model[0],\n",
        "            'worst_model': worst_model[0],\n",
        "            'production_ready': production_ready\n",
        "        },\n",
        "        'findings': findings,\n",
        "        'recommendations': recommendations\n",
        "    }\n",
        "\n",
        "# Generate final report\n",
        "final_report = generate_final_report(behavior_results, description_results, temporal_results, performance_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Results and Export Data {#export}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save evaluation results and export data\n",
        "def save_evaluation_results(behavior_results, description_results, temporal_results, performance_summary, final_report):\n",
        "    \"\"\"Save all evaluation results to files\"\"\"\n",
        "    \n",
        "    print(\"üíæ Saving Evaluation Results...\")\n",
        "    \n",
        "    # Create results directory\n",
        "    results_dir = 'evaluation_results'\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    \n",
        "    # Save individual model results\n",
        "    results_data = {\n",
        "        'behavior_results': behavior_results,\n",
        "        'description_results': description_results,\n",
        "        'temporal_results': temporal_results,\n",
        "        'performance_summary': performance_summary,\n",
        "        'final_report': final_report,\n",
        "        'evaluation_timestamp': datetime.now().isoformat(),\n",
        "        'device_used': str(device)\n",
        "    }\n",
        "    \n",
        "    # Save as pickle\n",
        "    with open(os.path.join(results_dir, 'rnn_evaluation_results.pkl'), 'wb') as f:\n",
        "        pickle.dump(results_data, f)\n",
        "    \n",
        "    # Save as JSON (convert numpy arrays to lists)\n",
        "    def convert_numpy(obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, dict):\n",
        "            return {k: convert_numpy(v) for k, v in obj.items()}\n",
        "        elif isinstance(obj, list):\n",
        "            return [convert_numpy(item) for item in obj]\n",
        "        else:\n",
        "            return obj\n",
        "    \n",
        "    json_data = convert_numpy(results_data)\n",
        "    with open(os.path.join(results_dir, 'rnn_evaluation_results.json'), 'w') as f:\n",
        "        json.dump(json_data, f, indent=2)\n",
        "    \n",
        "    # Create CSV summary\n",
        "    csv_data = []\n",
        "    for model_name, metrics in performance_summary.items():\n",
        "        csv_data.append({\n",
        "            'Model': model_name,\n",
        "            'Accuracy': metrics['accuracy'],\n",
        "            'Precision': metrics['precision'],\n",
        "            'Recall': metrics['recall'],\n",
        "            'F1_Score': metrics['f1'],\n",
        "            'Type': metrics['type'],\n",
        "            'Purpose': metrics['purpose']\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(csv_data)\n",
        "    df.to_csv(os.path.join(results_dir, 'model_performance_summary.csv'), index=False)\n",
        "    \n",
        "    # Create detailed confusion matrices\n",
        "    # User Behavior Confusion Matrix\n",
        "    cm_behavior_df = pd.DataFrame(\n",
        "        behavior_results['confusion_matrix'],\n",
        "        index=['search', 'upload', 'view', 'browse', 'logout'],\n",
        "        columns=['search', 'upload', 'view', 'browse', 'logout']\n",
        "    )\n",
        "    cm_behavior_df.to_csv(os.path.join(results_dir, 'user_behavior_confusion_matrix.csv'))\n",
        "    \n",
        "    # Temporal Pattern Confusion Matrix\n",
        "    time_periods = list(temporal_results['time_periods'].values())\n",
        "    time_labels = [period.split('(')[0].strip() for period in time_periods]\n",
        "    cm_temporal_df = pd.DataFrame(\n",
        "        temporal_results['confusion_matrix'],\n",
        "        index=time_labels,\n",
        "        columns=time_labels\n",
        "    )\n",
        "    cm_temporal_df.to_csv(os.path.join(results_dir, 'temporal_pattern_confusion_matrix.csv'))\n",
        "    \n",
        "    # Save model predictions\n",
        "    predictions_data = {\n",
        "        'user_behavior': {\n",
        "            'predictions': behavior_results['predictions'].tolist(),\n",
        "            'true_labels': behavior_results['true_labels'].tolist(),\n",
        "            'actions': ['search', 'upload', 'view', 'browse', 'logout']\n",
        "        },\n",
        "        'description': {\n",
        "            'predictions': description_results['predictions'].tolist(),\n",
        "            'true_labels': description_results['true_labels'].tolist(),\n",
        "            'descriptions': description_results['descriptions'][:50],  # First 50 descriptions\n",
        "            'categories': description_results['categories']\n",
        "        },\n",
        "        'temporal': {\n",
        "            'predictions': temporal_results['predictions'].tolist(),\n",
        "            'true_labels': temporal_results['true_labels'].tolist(),\n",
        "            'time_periods': temporal_results['time_periods']\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    with open(os.path.join(results_dir, 'model_predictions.json'), 'w') as f:\n",
        "        json.dump(predictions_data, f, indent=2)\n",
        "    \n",
        "    # Create a summary report\n",
        "    summary_report = f\"\"\"\n",
        "# BARYONYX RNN Model Evaluation Report\n",
        "\n",
        "**Evaluation Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "**Device Used:** {device}\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "- **Total Models Evaluated:** {final_report['summary']['total_models']}\n",
        "- **Average Accuracy:** {final_report['summary']['avg_accuracy']:.3f}\n",
        "- **Best Model:** {final_report['summary']['best_model']}\n",
        "- **Production Ready:** {final_report['summary']['production_ready']}/{final_report['summary']['total_models']} models\n",
        "\n",
        "## Model Performance\n",
        "\n",
        "| Model | Accuracy | Precision | Recall | F1-Score |\n",
        "|-------|----------|-----------|--------|----------|\n",
        "\"\"\"\n",
        "    \n",
        "    for model_name, metrics in performance_summary.items():\n",
        "        summary_report += f\"| {model_name} | {metrics['accuracy']:.4f} | {metrics['precision']:.4f} | {metrics['recall']:.4f} | {metrics['f1']:.4f} |\\n\"\n",
        "    \n",
        "    summary_report += f\"\"\"\n",
        "## Key Findings\n",
        "\n",
        "\"\"\"\n",
        "    for finding in final_report['findings']:\n",
        "        summary_report += f\"- {finding}\\n\"\n",
        "    \n",
        "    summary_report += f\"\"\"\n",
        "## Recommendations\n",
        "\n",
        "\"\"\"\n",
        "    for i, rec in enumerate(final_report['recommendations'], 1):\n",
        "        summary_report += f\"{i}. {rec}\\n\"\n",
        "    \n",
        "    summary_report += f\"\"\"\n",
        "## Files Generated\n",
        "\n",
        "- `rnn_evaluation_results.pkl` - Complete results (pickle format)\n",
        "- `rnn_evaluation_results.json` - Complete results (JSON format)\n",
        "- `model_performance_summary.csv` - Performance metrics summary\n",
        "- `user_behavior_confusion_matrix.csv` - User Behavior LSTM confusion matrix\n",
        "- `temporal_pattern_confusion_matrix.csv` - Temporal Pattern RNN confusion matrix\n",
        "- `model_predictions.json` - Detailed predictions for all models\n",
        "- `evaluation_summary.md` - This summary report\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. Review model performance and identify areas for improvement\n",
        "2. Implement recommended changes\n",
        "3. Set up continuous monitoring\n",
        "4. Deploy production-ready models\n",
        "5. Schedule regular re-evaluation\n",
        "\"\"\"\n",
        "    \n",
        "    with open(os.path.join(results_dir, 'evaluation_summary.md'), 'w') as f:\n",
        "        f.write(summary_report)\n",
        "    \n",
        "    print(f\"‚úÖ Results saved to {results_dir}/\")\n",
        "    print(f\"üìÅ Files created:\")\n",
        "    print(f\"   - rnn_evaluation_results.pkl\")\n",
        "    print(f\"   - rnn_evaluation_results.json\") \n",
        "    print(f\"   - model_performance_summary.csv\")\n",
        "    print(f\"   - user_behavior_confusion_matrix.csv\")\n",
        "    print(f\"   - temporal_pattern_confusion_matrix.csv\")\n",
        "    print(f\"   - model_predictions.json\")\n",
        "    print(f\"   - evaluation_summary.md\")\n",
        "    \n",
        "    return results_dir\n",
        "\n",
        "# Save all results\n",
        "results_directory = save_evaluation_results(behavior_results, description_results, temporal_results, performance_summary, final_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
